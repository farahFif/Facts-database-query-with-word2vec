{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Searching in fact database .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9+ZkOOyvrDXHeXMUr8aBU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farahFif/Facts-database-query-with-word2vec/blob/master/Searching_in_fact_database_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVqASylIbRg5",
        "colab_type": "text"
      },
      "source": [
        "# Searching in the curious facts database using word2vec and on topic modeling dataset \n",
        "\n",
        "We want you to retrieve facts relevant to the query, for example, you type \"good mood\", and get to know that Cherophobia is the fear of fun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cXp_lCEbQvq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "cae8f1ab-6e7c-4f61-89cd-7b8ae1d9709c"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.4)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.13.19)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.19 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.16.19)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "128tdpGIp_m1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7b5b07a0-e7e6-44a5-d8ae-490b9b20c9cc"
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec\n",
        "import pickle\n",
        "import os \n",
        "import re \n",
        "import numpy as np \n",
        "import nltk\n",
        "import nltk.tokenize as tokenizer\n",
        "nltk.download('punkt')\n",
        "from sklearn.metrics.pairwise import cosine_similarity \n",
        "from collections import Counter\n",
        "import heapq\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I27ifaHeqmXO",
        "colab_type": "text"
      },
      "source": [
        "We need first to read facts. Facts file is available  [here](https://github.com/hsu-ai-course/hsu.ai/blob/master/code/datasets/nlp/facts.txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-Egc5eNrLSj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "77e15c2f-afa3-4a3d-90de-14f8da69df62"
      },
      "source": [
        "# Read facts into list\n",
        "# facts can be found here https://github.com/hsu-ai-course/hsu.ai/blob/master/code/datasets/nlp/facts.txt\n",
        "\n",
        "file = open('facts.txt')\n",
        "facts = [ re.sub(r\"\\n\",\"\",x) for x in file.readlines()]\n",
        "print(facts[0],)\n",
        "file.close()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. If you somehow found a way to extract all of the gold from the bubbling core of our lovely little planet, you would be able to cover all of the land in a layer of gold up to your knees.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4NxDlPMq-NF",
        "colab_type": "text"
      },
      "source": [
        "For inferring vector with word2vec.\n",
        "First, let's load the pre-trained doc2vec model from https://github.com/jhlau/doc2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pROLrVDCsNFz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "834efc39-5c8e-4108-e1d9-4b0b31082e5d"
      },
      "source": [
        "#transforming sentences to vector\n",
        "\n",
        "def is_apt_word(word):\n",
        "  \"\"\" Checking if it is a word \"\"\"\n",
        "  return word.isalpha()\n",
        "\n",
        "def norm_vectors(A):\n",
        "    \"\"\" Normalizing vectors \"\"\"\n",
        "    An = A.copy()\n",
        "    norm = np.linalg.norm(An , axis=1).reshape(-1,1)\n",
        "    v = An/norm    \n",
        "    return An/norm\n",
        "\n",
        "words = []\n",
        "for i in range(len(facts)):\n",
        "  st = re.sub(r\"\\d+.\",\"\",facts[i])\n",
        "  tok  = tokenizer.word_tokenize(st)\n",
        "  words.append( [w for w in tok if is_apt_word(w)])\n",
        "print(words)\n",
        "\n",
        "# Generationg vectors\n",
        "fact_array = np.array(words)\n",
        "model = Doc2Vec.load('doc2vec.bin', mmap=None)\n",
        "sent_vecs = np.array([model.infer_vector(v) for v in fact_array])\n",
        "sent_vecs = norm_vectors(sent_vecs)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['If', 'you', 'somehow', 'found', 'a', 'way', 'to', 'extract', 'all', 'of', 'the', 'gold', 'from', 'the', 'bubbling', 'core', 'of', 'our', 'lovely', 'little', 'planet', 'you', 'would', 'be', 'able', 'to', 'cover', 'all', 'of', 'the', 'land', 'in', 'a', 'layer', 'of', 'gold', 'up', 'to', 'your', 'knees'], ['McDonalds', 'calls', 'frequent', 'buyers', 'of', 'their', 'food', 'heavy', 'users'], ['The', 'average', 'person', 'spends', 'months', 'of', 'their', 'lifetime', 'waiting', 'on', 'a', 'red', 'light', 'to', 'turn', 'green'], ['The', 'largest', 'recorded', 'snowflake', 'was', 'in', 'Keogh', 'MT', 'during', 'year', 'and', 'was', 'inches', 'wide'], ['You', 'burn', 'more', 'calories', 'sleeping', 'than', 'you', 'do', 'watching', 'television'], ['There', 'are', 'more', 'lifeforms', 'living', 'on', 'your', 'skin', 'than', 'there', 'are', 'people', 'on', 'the', 'planet'], ['Southern', 'sea', 'otters', 'have', 'flaps', 'of', 'skin', 'under', 'their', 'forelegs', 'that', 'act', 'as', 'pockets', 'When', 'diving', 'they', 'use', 'these', 'pouches', 'to', 'store', 'rocks', 'and', 'food'], ['In', 'a', 'pig', 'in', 'France', 'was', 'executed', 'by', 'public', 'hanging', 'for', 'the', 'murder', 'of', 'a', 'child'], ['One', 'in', 'every', 'five', 'adults', 'believe', 'that', 'aliens', 'are', 'hiding', 'in', 'our', 'planet', 'disguised', 'as', 'humans'], ['If', 'you', 'believe', 'that', 'you', 're', 'truly', 'one', 'in', 'a', 'million', 'there', 'are', 'still', 'approximately', 'more', 'people', 'out', 'there', 'just', 'like', 'you'], ['A', 'single', 'cloud', 'can', 'weight', 'more', 'than', 'million', 'pounds'], ['A', 'human', 'will', 'eat', 'on', 'average', 'assorted', 'insects', 'and', 'spiders', 'while', 'sleeping'], ['James', 'Buchanan', 'the', 'h', 'president', 'continuously', 'bought', 'slaves', 'with', 'his', 'own', 'money', 'in', 'order', 'to', 'free', 'them'], ['There', 'are', 'more', 'possible', 'iterations', 'of', 'a', 'game', 'of', 'chess', 'than', 'there', 'are', 'atoms', 'in', 'the', 'known', 'universe'], ['The', 'average', 'person', 'walks', 'the', 'equivalent', 'of', 'three', 'times', 'around', 'the', 'world', 'in', 'a', 'lifetime'], ['Men', 'are', 'times', 'more', 'likely', 'to', 'be', 'struck', 'by', 'lightning', 'than', 'women'], ['would', 'be', 'green', 'if', 'coloring', 'wasn', 't', 'added', 'to', 'it'], ['You', 'can', 'not', 'snore', 'and', 'dream', 'at', 'the', 'same', 'time'], ['The', 'world', 's', 'oldest', 'piece', 'of', 'chewing', 'gum', 'is', 'over', 'years', 'old'], ['A', 'coyote', 'can', 'hear', 'a', 'mouse', 'moving', 'underneath', 'a', 'foot', 'of', 'snow'], ['Bolts', 'of', 'lightning', 'can', 'shoot', 'out', 'of', 'an', 'erupting', 'volcano'], ['New', 'York', 'drifts', 'about', 'one', 'inch', 'farther', 'away', 'from', 'London', 'each', 'year'], ['A', 'dollar', 'bill', 'can', 'be', 'folded', 'approximately', 'times', 'in', 'the', 'same', 'place', 'before', 'it', 'will', 'tear'], ['A', 'sneeze', 'travels', 'about', 'miles', 'per', 'hour'], ['Earth', 'has', 'traveled', 'more', 'than', 'miles', 'in', 'the', 'past', 'minutes'], ['It', 'would', 'take', 'a', 'sloth', 'one', 'month', 'to', 'travel', 'one', 'mile'], ['of', 'the', 'World', 's', 'population', 'is', 'left', 'handed'], ['A', 'broken', 'clock', 'is', 'right', 'two', 'times', 'every', 'day'], ['According', 'to', 'Amazon', 'the', 'most', 'highlighted', 'books', 'on', 'Kindle', 'are', 'the', 'Bible', 'the', 'Steve', 'Jobs', 'biography', 'and', 'The', 'Hunger', 'Games'], ['Bob', 'Marley', 's', 'last', 'words', 'to', 'his', 'son', 'before', 'he', 'died', 'were', 'Money', 'can', 't', 'buy', 'life'], ['A', 'mole', 'can', 'dig', 'a', 'tunnel', 'that', 'is', 'feet', 'long', 'in', 'only', 'one', 'night'], ['A', 'hippo', 's', 'wide', 'open', 'mouth', 'is', 'big', 'enough', 'to', 'fit', 'a', 'child', 'in'], ['Chewing', 'gum', 'while', 'you', 'cut', 'an', 'onion', 'will', 'help', 'keep', 'you', 'from', 'crying'], ['If', 'you', 'were', 'to', 'stretch', 'a', 'Slinky', 'out', 'until', 'it', 's', 'flat', 'it', 'would', 'measure', 'feet', 'long'], ['Al', 'Capone', 's', 'business', 'card', 'said', 'he', 'was', 'a', 'used', 'furniture', 'dealer'], ['There', 'are', 'more', 'collect', 'calls', 'on', 'Father', 's', 'Day', 'than', 'on', 'any', 'other', 'day', 'of', 'the', 'year'], ['Banging', 'your', 'head', 'against', 'a', 'wall', 'burns', 'calories', 'an', 'hour'], ['of', 'people', 'text', 'things', 'they', 'could', 'never', 'say', 'in', 'person'], ['A', 'crocodile', 'can', 't', 'poke', 'its', 'tongue', 'out'], ['It', 'is', 'physically', 'impossible', 'for', 'pigs', 'to', 'look', 'up', 'into', 'the', 'sky'], ['Guinness', 'Book', 'of', 'Records', 'holds', 'the', 'record', 'for', 'being', 'the', 'book', 'most', 'often', 'stolen', 'from', 'Public', 'Libraries'], ['Drying', 'fruit', 'depletes', 'it', 'of', 'of', 'its', 'vitamin', 'and', 'antioxidant', 'content'], ['A', 'study', 'found', 'that', 'of', 'soda', 'fountain', 'contained', 'fecal', 'bacteria', 'and', 'contained', 'Coli'], ['out', 'of', 'Americans', 'are', 'deficient', 'in', 'Potassium'], ['Blueberries', 'will', 'not', 'ripen', 'until', 'they', 'are', 'picked'], ['About', 'people', 'per', 'year', 'are', 'killed', 'by', 'coconuts'], ['Ketchup', 'was', 'used', 'as', 'a', 'medicine', 'back', 'in', 'the', 's'], ['Honey', 'never', 'spoils'], ['About', 'half', 'of', 'all', 'Americans', 'are', 'on', 'a', 'diet', 'on', 'any', 'given', 'day'], ['A', 'hardboiled', 'egg', 'will', 'spin', 'but', 'a', 'egg', 'will', 'not'], ['Avocados', 'are', 'poisonous', 'to', 'birds'], ['Chewing', 'gum', 'burns', 'about', 'calories', 'per', 'hour'], ['The', 'number', 'of', 'animals', 'killed', 'for', 'meat', 'every', 'hour', 'in', 'the', 'is'], ['If', 'you', 'try', 'to', 'suppress', 'a', 'sneeze', 'you', 'can', 'rupture', 'a', 'blood', 'vessel', 'in', 'your', 'head', 'or', 'neck', 'and', 'die'], ['Celery', 'has', 'negative', 'calories', 'It', 'takes', 'more', 'calories', 'to', 'eat', 'a', 'piece', 'of', 'celery', 'than', 'the', 'celery', 'has', 'in', 'it', 'to', 'begin', 'with', 'It', 's', 'the', 'same', 'with', 'apples'], ['More', 'people', 'are', 'allergic', 'to', 'cow', 's', 'milk', 'than', 'any', 'other', 'food'], ['Only', 'of', 'dieters', 'will', 'follow', 'a', 'restrictive', 'weight', 'loss', 'plan', 'such', 'as', 'hCG', 'Drops', 'diet', 'garcinia', 'cambogia', 'diet', 'etc'], ['Coconut', 'water', 'can', 'be', 'used', 'as', 'blood', 'plasma'], ['The', 'word', 'gorilla', 'is', 'derived', 'from', 'a', 'Greek', 'word', 'meaning', 'A', 'tribe', 'of', 'hairy', 'women'], ['Prisoners', 'in', 'Canadian', 'war', 'camps', 'during', 'WWII', 'were', 'treated', 'so', 'well', 'that', 'a', 'lot', 'of', 'them', 'didn', 't', 'want', 'to', 'leave', 'when', 'the', 'war', 'was', 'over'], ['Gorillas', 'burp', 'when', 'they', 'are', 'happy'], ['In', 'New', 'York', 'it', 'is', 'illegal', 'to', 'sell', 'a', 'haunted', 'house', 'without', 'telling', 'the', 'buyer'], ['In', 'someone', 'tried', 'to', 'sell', 'New', 'Zealand', 'on', 'eBay', 'The', 'price', 'got', 'up', 'to', 'before', 'eBay', 'shut', 'it', 'down'], ['It', 'is', 'considered', 'good', 'luck', 'in', 'Japan', 'when', 'a', 'sumo', 'wrestler', 'makes', 'your', 'baby', 'cry'], ['A', 'man', 'from', 'Britain', 'changed', 'his', 'name', 'to', 'Tim', 'Pppppppppprice', 'to', 'make', 'it', 'harder', 'for', 'telemarketers', 'to', 'pronounce'], ['A', 'woman', 'from', 'California', 'once', 'tried', 'to', 'sue', 'the', 'makers', 'of', 'Cap', 'n', 'Crunch', 'because', 'the', 'Crunch', 'Berries', 'contained', 'no', 'berries', 'of', 'any', 'kind'], ['Apple', 'launched', 'a', 'clothing', 'line', 'in', 'It', 'was', 'described', 'as', 'a', 'train', 'wreck', 'by', 'others'], ['In', 'Japan', 'crooked', 'teeth', 'are', 'considered', 'cute', 'and', 'attractive'], ['A', 'Swedish', 'woman', 'lost', 'her', 'wedding', 'ring', 'and', 'found', 'it', 'years', 'growing', 'on', 'a', 'carrot', 'in', 'her', 'garden'], ['Donald', 'duck', 'comics', 'were', 'banned', 'from', 'Finland', 'because', 'he', 'doesn', 't', 'wear', 'pants'], ['The', 'chance', 'of', 'you', 'dying', 'on', 'the', 'way', 'to', 'get', 'lottery', 'tickets', 'is', 'actually', 'greater', 'than', 'your', 'chance', 'of', 'winning'], ['Cherophobia', 'is', 'the', 'fear', 'of', 'fun'], ['The', 'toothpaste', 'Colgate', 'in', 'Spanish', 'translates', 'to', 'go', 'hang', 'yourself'], ['Pirates', 'wore', 'earrings', 'because', 'they', 'believed', 'it', 'improved', 'their', 'eyesight'], ['Human', 'thigh', 'bones', 'are', 'stronger', 'than', 'concrete'], ['Cockroaches', 'can', 'live', 'for', 'several', 'weeks', 'with', 'their', 'heads', 'cut', 'off', 'because', 'their', 'brains', 'are', 'located', 'inside', 'their', 'body', 'They', 'would', 'eventually', 'die', 'from', 'being', 'unable', 'to', 'eat'], ['Scientists', 'have', 'tracked', 'butterflies', 'that', 'travel', 'over', 'miles'], ['To', 'produce', 'a', 'single', 'pound', 'of', 'honey', 'a', 'single', 'bee', 'would', 'have', 'to', 'visit', 'million', 'flowers'], ['The', 'population', 'is', 'expected', 'to', 'rise', 'to', 'billion', 'by', 'the', 'year'], ['You', 'breathe', 'on', 'average', 'about', 'times', 'a', 'year'], ['More', 'than', 'people', 'are', 'flying', 'over', 'the', 'United', 'States', 'in', 'an', 'airplane', 'right', 'now'], ['Hamsters', 'run', 'up', 'to', 'miles', 'at', 'night', 'on', 'a', 'wheel'], ['A', 'waterfall', 'in', 'Hawaii', 'goes', 'up', 'sometimes', 'instead', 'of', 'down'], ['A', 'church', 'in', 'the', 'Czech', 'Republic', 'has', 'a', 'chandelier', 'made', 'entirely', 'of', 'human', 'bones'], ['Under', 'the', 'Code', 'of', 'Hammurabi', 'bartenders', 'who', 'watered', 'down', 'beer', 'were', 'punished', 'by', 'execution'], ['Our', 'eyes', 'are', 'always', 'the', 'same', 'size', 'from', 'birth', 'but', 'our', 'nose', 'and', 'ears', 'never', 'stop', 'growing'], ['During', 'your', 'lifetime', 'you', 'will', 'produce', 'enough', 'saliva', 'to', 'fill', 'two', 'swimming', 'pools'], ['You', 'are', 'shorter', 'in', 'the', 'evening', 'than', 'in', 'the', 'morning'], ['The', 'elephant', 'is', 'the', 'only', 'mammal', 'that', 'can', 't', 'jump'], ['Most', 'dust', 'particles', 'in', 'your', 'house', 'are', 'made', 'from', 'dead', 'skin'], ['If', 'million', 'people', 'held', 'hands', 'they', 'could', 'make', 'it', 'all', 'the', 'way', 'around', 'the', 'equator'], ['Earth', 'is', 'the', 'only', 'planet', 'that', 'is', 'not', 'named', 'after', 'a', 'god'], ['The', 'bloodhound', 'is', 'the', 'only', 'animal', 'whose', 'evidence', 'is', 'admissible', 'in', 'court'], ['You', 'are', 'born', 'with', 'bones', 'but', 'by', 'the', 'time', 'you', 'are', 'an', 'adult', 'you', 'only', 'have'], ['A', 'hat', 'will', 'only', 'hold', 'of', 'a', 'gallon'], ['Just', 'like', 'fingerprints', 'everyone', 'has', 'different', 'tongue', 'prints'], ['ATM', 's', 'were', 'originally', 'thought', 'to', 'be', 'failures', 'because', 'the', 'only', 'users', 'were', 'prostitutes', 'and', 'gamblers', 'who', 'didn', 't', 'want', 'to', 'deal', 'with', 'tellers', 'face', 'to', 'face'], ['Of', 'all', 'the', 'words', 'in', 'the', 'English', 'language', 'the', 'word', 'set', 'has', 'the', 'most', 'definitions', 'The', 'word', 'run', 'comes', 'in', 'close', 'second'], ['A', 'jiffy', 'is', 'the', 'scientific', 'name', 'for', 'h', 'of', 'a', 'second'], ['One', 'fourth', 'of', 'the', 'bones', 'in', 'your', 'body', 'are', 'located', 'in', 'your', 'feet'], ['X'], ['people', 'tend', 'to', 'have', 'the', 'highest', 'tolerance', 'of', 'alcohol'], ['A', 'traffic', 'jam', 'lasted', 'for', 'more', 'than', 'days', 'with', 'cars', 'only', 'moving', 'miles', 'a', 'day'], ['The', 'tongue', 'is', 'the', 'strongest', 'muscle', 'in', 'the', 'body'], ['Every', 'year', 'more', 'than', 'people', 'are', 'killed', 'from', 'using', 'products'], ['More', 'than', 'of', 'the', 'people', 'in', 'the', 'world', 'have', 'never', 'made', 'or', 'received', 'a', 'telephone', 'call'], ['The', 'cigarette', 'lighter', 'was', 'invented', 'before', 'the', 'match'], ['Sea', 'otters', 'hold', 'hands', 'when', 'they', 'sleep', 'so', 'that', 'they', 'do', 'not', 'drift', 'apart'], ['The', 'Golden', 'Poison', 'Dart', 'Frog', 's', 'skin', 'has', 'enough', 'toxins', 'to', 'kill', 'people'], ['The', 'male', 'ostrich', 'can', 'roar', 'just', 'like', 'a', 'lion'], ['Mountain', 'lions', 'can', 'whistle'], ['The', 'giraffe', 's', 'tongue', 'is', 'so', 'long', 'that', 'they', 'can', 'lick', 'the', 'inside', 'of', 'their', 'own', 'ear'], ['Cows', 'kill', 'more', 'people', 'than', 'sharks', 'do'], ['Cats', 'have', 'muscles', 'in', 'each', 'of', 'their', 'ears'], ['Butterflies', 'taste', 'their', 'food', 'with', 'their', 'feet'], ['A', 'tarantula', 'can', 'live', 'without', 'food', 'for', 'more', 'than', 'two', 'years'], ['The', 'tongue', 'of', 'a', 'blue', 'whale', 'weighs', 'more', 'than', 'most', 'elephants'], ['Ever', 'wonder', 'where', 'the', 'phrase', 'It', 's', 'raining', 'cats', 'and', 'dogs', 'comes', 'from', 'In', 'the', 'h', 'century', 'many', 'homeless', 'cats', 'and', 'dogs', 'would', 'drown', 'and', 'float', 'down', 'the', 'streets', 'of', 'England', 'making', 'it', 'look', 'like', 'it', 'literally', 'rained', 'cats', 'and', 'dogs'], ['It', 'takes', 'about', 'cows', 'to', 'supply', 'enough', 'leather', 'for', 'the', 'NFL', 'for', 'only', 'one', 'year'], ['Male', 'dogs', 'lift', 'their', 'legs', 'when', 'they', 'are', 'urinating', 'for', 'a', 'reason', 'They', 'are', 'trying', 'to', 'leave', 'their', 'mark', 'higher', 'so', 'that', 'it', 'gives', 'off', 'the', 'message', 'that', 'they', 'are', 'tall', 'and', 'intimidating'], ['A', 'hummingbird', 'weighs', 'less', 'than', 'a', 'penny'], ['An', 'ostrich', 's', 'eye', 'is', 'bigger', 'than', 'its', 'brain'], ['Dogs', 'are', 'capable', 'of', 'understanding', 'up', 'to', 'words', 'and', 'gestures', 'and', 'have', 'demonstrated', 'the', 'ability', 'to', 'do', 'simple', 'mathematical', 'calculations'], ['A', 'sheep', 'a', 'duck', 'and', 'a', 'rooster', 'were', 'the', 'first', 'passengers', 'in', 'a', 'hot', 'air', 'balloon'], ['Birds', 'don', 't', 'urinate'], ['A', 'flea', 'can', 'jump', 'up', 'to', 'times', 'its', 'own', 'height', 'That', 'is', 'the', 'equivalent', 'of', 'a', 'human', 'jumping', 'the', 'Empire', 'State', 'Building'], ['There', 'are', 'temples', 'in', 'Kyoto', 'Japan', 'that', 'have', 'blood', 'stained', 'ceilings', 'The', 'ceilings', 'are', 'made', 'from', 'the', 'floorboards', 'of', 'a', 'castle', 'where', 'warriors', 'killed', 'themselves', 'after', 'a', 'long', 'against', 'an', 'army', 'To', 'this', 'day', 'you', 'can', 'still', 'see', 'the', 'outlines', 'and', 'footprints'], ['There', 'is', 'a', 'snake', 'called', 'the', 'boomslang', 'whose', 'venom', 'causes', 'you', 'to', 'bleed', 'out', 'from', 'every', 'orifice', 'on', 'your', 'body', 'You', 'may', 'even', 'turn', 'blue', 'from', 'internal', 'bleeding', 'and', 'it', 'can', 'take', 'up', 'to', 'days', 'to', 'die', 'from', 'the', 'bleeding'], ['A', 'ball', 'of', 'glass', 'will', 'bounce', 'higher', 'than', 'a', 'ball', 'of', 'rubber'], ['Saturn', 's', 'density', 'is', 'low', 'enough', 'that', 'the', 'planet', 'would', 'float', 'in', 'water'], ['of', 'the', 'universe', 'is', 'dark', 'energy', 'and', 'is', 'dark', 'matter', 'both', 'are', 'invisible', 'even', 'with', 'our', 'powerful', 'telescopes', 'This', 'means', 'we', 'have', 'only', 'seen', 'of', 'the', 'universe', 'from', 'earth'], ['The', 'founders', 'of', 'Google', 'were', 'willing', 'to', 'sell', 'Google', 'for', 'million', 'to', 'Excite', 'in', 'but', 'Excite', 'turned', 'them', 'down', 'Google', 'is', 'now', 'worth', 'Billion'], ['In', 'the', 'past', 'years', 'scientists', 'have', 'found', 'over', 'planets', 'outside', 'of', 'our', 'solar', 'system'], ['There', 'are', 'miles', 'of', 'blood', 'vessels', 'in', 'the', 'human', 'body'], ['If', 'a', 'pregnant', 'woman', 'has', 'organ', 'damage', 'the', 'baby', 'in', 'her', 'womb', 'sends', 'stem', 'cells', 'to', 'help', 'repair', 'the', 'organ'], ['If', 'you', 'started', 'with', 'and', 'doubled', 'your', 'money', 'every', 'day', 'it', 'would', 'take', 'days', 'to', 'become', 'a', 'millionaire'], ['Only', 'one', 'person', 'in', 'two', 'billion', 'will', 'live', 'to', 'be', 'or', 'older'], ['A', 'person', 'can', 'live', 'without', 'food', 'for', 'about', 'a', 'month', 'but', 'only', 'about', 'a', 'week', 'without', 'water', 'If', 'the', 'amount', 'of', 'water', 'in', 'your', 'body', 'is', 'reduced', 'by', 'just', 'you', 'll', 'feel', 'thirsty', 'If', 'it', 's', 'reduced', 'by', 'you', 'll', 'die'], ['On', 'average', 'newborns', 'will', 'be', 'given', 'to', 'the', 'wrong', 'parents', 'daily'], ['You', 'can', 't', 'kill', 'yourself', 'by', 'holding', 'your', 'breath'], ['Human', 'birth', 'control', 'pills', 'work', 'on', 'gorillas'], ['There', 'are', 'no', 'clocks', 'in', 'Las', 'Vegas', 'gambling', 'casinos'], ['Beetles', 'taste', 'like', 'apples', 'wasps', 'like', 'pine', 'nuts', 'and', 'worms', 'like', 'fried', 'bacon'], ['What', 'is', 'called', 'a', 'French', 'kiss', 'in', 'the', 'world', 'is', 'known', 'as', 'an', 'English', 'kiss', 'in', 'France'], ['Months', 'that', 'begin', 'on', 'a', 'Sunday', 'will', 'always', 'have', 'a', 'Friday', 'the', 'h'], ['The', 'placement', 'of', 'a', 'donkey', 's', 'eyes', 'in', 'its', 'heads', 'enables', 'it', 'to', 'see', 'all', 'four', 'feet', 'at', 'all', 'times'], ['Some', 'worms', 'will', 'eat', 'themselves', 'if', 'they', 'can', 't', 'find', 'any', 'food'], ['Dolphins', 'sleep', 'with', 'one', 'eye', 'open'], ['It', 'is', 'impossible', 'to', 'sneeze', 'with', 'your', 'eyes', 'open'], ['In', 'France', 'it', 'is', 'legal', 'to', 'marry', 'a', 'dead', 'person'], ['Russia', 'has', 'a', 'larger', 'surface', 'area', 'than', 'Pluto'], ['There', 's', 'an', 'opera', 'house', 'on', 'the', 'border', 'where', 'the', 'stage', 'is', 'in', 'one', 'country', 'and', 'half', 'the', 'audience', 'is', 'in', 'another'], ['The', 'harder', 'you', 'concentrate', 'on', 'falling', 'asleep', 'the', 'less', 'likely', 'to', 'fall', 'asleep'], ['You', 'can', 't', 'hum', 'while', 'holding', 'your', 'nose', 'closed'], ['Women', 'have', 'twice', 'as', 'many', 'pain', 'receptors', 'on', 'their', 'body', 'than', 'men', 'But', 'a', 'much', 'higher', 'pain', 'tolerance'], ['There', 'are', 'more', 'stars', 'in', 'space', 'than', 'there', 'are', 'grains', 'of', 'sand', 'on', 'every', 'beach', 'in', 'the', 'world'], ['For', 'every', 'human', 'on', 'Earth', 'there', 'are', 'million', 'ants'], ['The', 'total', 'weight', 'of', 'all', 'those', 'ants', 'however', 'is', 'about', 'the', 'same', 'as', 'all', 'the', 'humans'], ['On', 'Jupiter', 'and', 'Saturn', 'it', 'rains', 'diamonds']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/doc2vec.py:566: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
            "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
            "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cygbn7oOrRdL",
        "colab_type": "text"
      },
      "source": [
        "Then we need to find 5 closest facts to the query. We need to calculate cosine similarity between query vector and vectors from facts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvcMJFb1trTR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "26630985-100e-4c2c-aed8-619b71758ec9"
      },
      "source": [
        "def get_words_from_sentence(sentences):\n",
        "    for sentence in sentences: \n",
        "        yield nltk.word_tokenize(sentence.split('.', 1)[1])\n",
        "\n",
        "def find_k_closest(query, dataset, k=5):    \n",
        "    #find 5 closest rows in dataset in terms of cosine similarity\n",
        "    #Since vectors in dataset are already normed, cosine similarity is just dot product.  \n",
        "    op = []\n",
        "    for i in range(len(dataset)):\n",
        "      op.append((i, np.dot(query, dataset[i])))\n",
        "          \n",
        "    cos = np.asarray([tup[1] for tup in op])\n",
        "    indx = heapq.nlargest(5, range(len(cos)), cos.take)\n",
        "    sc = [op[j] for j in indx]\n",
        "    return sc\n",
        "\n",
        "\n",
        "query = \"good mood\"\n",
        "query_vec = model.infer_vector(nltk.word_tokenize(query))\n",
        "query_vec_normed = query_vec/np.linalg.norm(query_vec)\n",
        "r = find_k_closest(query_vec_normed,sent_vecs)\n",
        "\n",
        "print(\"Results for query:\", query)\n",
        "for k, p in r:\n",
        "    print(\"\\t\", facts[k], \"sim=\", p)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results for query: good mood\n",
            "\t 144. Dolphins sleep with one eye open! sim= 0.6115808\n",
            "\t 68. Cherophobia is the fear of fun. sim= 0.60771847\n",
            "\t 57. Gorillas burp when they are happy sim= 0.59873986\n",
            "\t 76. You breathe on average about 8,409,600 times a year sim= 0.5648149\n",
            "\t 110. Cats have 32 muscles in each of their ears. sim= 0.56407446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvHEWfqdt6ZT",
        "colab_type": "text"
      },
      "source": [
        "# Training doc2vec model on topic-modeling dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJaJ5FIet5pC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "ff1a8af5-5498-4303-ce54-4ff680c301ff"
      },
      "source": [
        "# first we download the dataset that consists of 4 files each file has one specific topic\n",
        "\n",
        "! wget 'https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/topic-modeling-tool/testdata_news_music_2084docs.txt'\n",
        "! wget 'https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/topic-modeling-tool/testdata_news_economy_2073docs.txt'\n",
        "! wget 'https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/topic-modeling-tool/testdata_news_fuel_845docs.txt'\n",
        "! wget 'https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/topic-modeling-tool/testdata_braininjury_10000docs.txt'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-04 15:10:18--  https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/topic-modeling-tool/testdata_news_music_2084docs.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.23.128, 2404:6800:4008:c01::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.23.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13985603 (13M) [application/octet-stream]\n",
            "Saving to: ‘testdata_news_music_2084docs.txt’\n",
            "\n",
            "testdata_news_music 100%[===================>]  13.34M  19.3MB/s    in 0.7s    \n",
            "\n",
            "2020-06-04 15:10:19 (19.3 MB/s) - ‘testdata_news_music_2084docs.txt’ saved [13985603/13985603]\n",
            "\n",
            "--2020-06-04 15:10:21--  https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/topic-modeling-tool/testdata_news_economy_2073docs.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.23.128, 2404:6800:4008:c01::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.23.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13682532 (13M) [application/octet-stream]\n",
            "Saving to: ‘testdata_news_economy_2073docs.txt’\n",
            "\n",
            "testdata_news_econo 100%[===================>]  13.05M  16.1MB/s    in 0.8s    \n",
            "\n",
            "2020-06-04 15:10:22 (16.1 MB/s) - ‘testdata_news_economy_2073docs.txt’ saved [13682532/13682532]\n",
            "\n",
            "--2020-06-04 15:10:24--  https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/topic-modeling-tool/testdata_news_fuel_845docs.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 2404:6800:4008:c01::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5564877 (5.3M) [application/octet-stream]\n",
            "Saving to: ‘testdata_news_fuel_845docs.txt’\n",
            "\n",
            "testdata_news_fuel_ 100%[===================>]   5.31M  21.2MB/s    in 0.3s    \n",
            "\n",
            "2020-06-04 15:10:25 (21.2 MB/s) - ‘testdata_news_fuel_845docs.txt’ saved [5564877/5564877]\n",
            "\n",
            "--2020-06-04 15:10:27--  https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/topic-modeling-tool/testdata_braininjury_10000docs.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 2404:6800:4008:c01::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10096125 (9.6M) [application/octet-stream]\n",
            "Saving to: ‘testdata_braininjury_10000docs.txt’\n",
            "\n",
            "testdata_braininjur 100%[===================>]   9.63M  14.8MB/s    in 0.7s    \n",
            "\n",
            "2020-06-04 15:10:28 (14.8 MB/s) - ‘testdata_braininjury_10000docs.txt’ saved [10096125/10096125]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i5LusmquNGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_dataset(file_path):\n",
        "    docs = []\n",
        "    with open(file_path) as fp:\n",
        "        for cnt, line in enumerate(fp):\n",
        "            docs.append(nltk.word_tokenize(line))\n",
        "    return docs\n",
        "\n",
        "fuel_data = read_dataset(\"testdata_news_fuel_845docs.txt\")\n",
        "brain_inj_data = read_dataset(\"testdata_braininjury_10000docs.txt\")\n",
        "economy_data = read_dataset(\"testdata_news_economy_2073docs.txt\")\n",
        "music_data = read_dataset(\"testdata_news_music_2084docs.txt\")\n",
        "\n",
        "all_data = fuel_data + brain_inj_data + economy_data + music_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPbYhzitxJz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "267506a6-4875-45e7-cf73-5a3f94282470"
      },
      "source": [
        "print(len(all_data))\n",
        "assert len(all_data) == 15002"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anZrBiySwaSM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "13c1f9b8-643c-407b-b515-aa15fa6e04ff"
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# just a test set of tokenized sentences\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_data)]\n",
        "\n",
        "# train a model\n",
        "model_d2v = Doc2Vec(\n",
        "    documents,     # collection of texts\n",
        "    vector_size=300, # output vector size\n",
        "    window=2,      # maximum distance between the target word and its neighboring word\n",
        "    min_count=1,   # minimal number of \n",
        "    workers=4      # in parallel\n",
        ")\n",
        "\n",
        "# clean training data\n",
        "model_d2v.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
        "\n",
        "# save and load\n",
        "model_d2v.save(\"d2v.model\")\n",
        "model_d2v = Doc2Vec.load(\"d2v.model\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKWtPeBXyHcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# organazing labels\n",
        "all_labels = np.zeros((len(all_data)))\n",
        "all_labels[:len(fuel_data)] = 1\n",
        "all_labels[len(fuel_data):len(fuel_data) + len(brain_inj_data)] = 2\n",
        "all_labels[len(fuel_data) + len(brain_inj_data): len(fuel_data) + len(brain_inj_data) + len(economy_data)] = 3     \n",
        "\n",
        "# transforming data to vectors\n",
        "all_data_vecs = np.array(list(model_d2v.infer_vector(sent) for sent in all_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xROrkZpmzwHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import utils\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_data_vecs = np.array(list(model_d2v.infer_vector(sent) for sent in all_data))\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_data_vecs, all_labels, test_size=0.33, \n",
        "                                                    random_state=0, stratify=all_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ3YfGqj1Ndt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "d0af0cff-59ae-424c-83d0-f39837ce3926"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# SVM\n",
        "\n",
        "clf = LinearSVC(random_state=0, tol=1e-5)\n",
        "clf.fit(X_train, y_train)\n",
        "target_names = [\"music\", \"fuel\", \"brain\", \"economy\"] # (0,1,2,3)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names = target_names))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       music       0.79      0.89      0.83       688\n",
            "        fuel       0.67      0.36      0.47       279\n",
            "       brain       1.00      1.00      1.00      3300\n",
            "     economy       0.76      0.80      0.78       684\n",
            "\n",
            "    accuracy                           0.92      4951\n",
            "   macro avg       0.80      0.76      0.77      4951\n",
            "weighted avg       0.92      0.92      0.92      4951\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}